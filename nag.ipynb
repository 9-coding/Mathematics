{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNV6HotPKZ5G9dNe2ffR62P"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-J6PsFRDu5T"
      },
      "outputs": [],
      "source": [
        "# Reference: https://stackabuse.com/gradient-descent-in-python-implementation-and-theory/\n",
        "# A code that converges which is optimal by changing mu, \n",
        "# one of the parameters of NAG, from 0 to 1 to 0.1 units.\n",
        "# Includes convergent visualization.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.datasets as dt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Make threshold a -ve value if you want to run exactly\n",
        "# max_iterations.\n",
        "\n",
        "# visualision_fw() function : generates 2500 points at equal intervals and calculates the function value at each point.\n",
        "def visualize_fw():\n",
        "    xcoord = np.linspace(-10.0,10.0,50)\n",
        "    ycoord = np.linspace(-10.0,10.0,50)\n",
        "    w1,w2 = np.meshgrid(xcoord,ycoord)\n",
        "    pts = np.vstack((w1.flatten(),w2.flatten()))\n",
        "    \n",
        "    # All 2D points on the grid\n",
        "    pts = pts.transpose()\n",
        "    \n",
        "    # Function value at each point\n",
        "    f_vals = np.sum(pts*pts,axis=1)\n",
        "    function_plot(pts,f_vals)\n",
        "    plt.title('Objective Function Shown in Color')\n",
        "    plt.show()\n",
        "    return pts,f_vals\n",
        "\n",
        "# Helper function to annotate a single point\n",
        "# Visualize points through color and text\n",
        "def annotate_pt(text,xy,xytext,color):\n",
        "    plt.plot(xy[0],xy[1],marker='P',markersize=10,c=color)\n",
        "    plt.annotate(text,xy=xy,xytext=xytext,\n",
        "                 arrowprops=dict(arrowstyle=\"->\",\n",
        "                 color = color,\n",
        "                 connectionstyle='arc3'))\n",
        "\n",
        "# function_plot() function : displays all points in different colors depending on the f(w) value of the point.\n",
        "def function_plot(pts,f_val):\n",
        "    f_plot = plt.scatter(pts[:,0],pts[:,1],\n",
        "                         c=f_val,vmin=min(f_val),vmax=max(f_val),\n",
        "                         cmap='RdBu_r')\n",
        "    plt.colorbar(f_plot)\n",
        "    # Show the optimal point\n",
        "    annotate_pt('global minimum',(0,0),(-5,-7),'yellow')    \n",
        "\n",
        "pts,f_vals = visualize_fw()\n",
        "\n",
        "\n",
        "# Running Gradient Descent with Different Hyper-parameters\n",
        "\n",
        "# Objective function\n",
        "def f(w,extra=[]):\n",
        "    return np.sum(w*w)\n",
        "\n",
        "# Function to compute the gradient\n",
        "def grad(w,extra=[]):\n",
        "    return 2*w\n",
        "\n",
        "# Function to plot the objective function\n",
        "# and learning history annotated by arrows\n",
        "# to show how learning proceeded\n",
        "# The arrows on the graph allow you to easily track the last updated point\n",
        "def visualize_learning(w_history):  \n",
        "    \n",
        "    # Make the function plot\n",
        "    function_plot(pts,f_vals)\n",
        "    \n",
        "    # Plot the history\n",
        "    plt.plot(w_history[:,0],w_history[:,1],marker='o',c='magenta') \n",
        "    \n",
        "    # Annotate the point found at last iteration\n",
        "    annotate_pt('minimum found',\n",
        "                (w_history[-1,0],w_history[-1,1]),\n",
        "                (-1,7),'green')\n",
        "    iter = w_history.shape[0]    \n",
        "\n",
        "\n",
        "#Gradient Descent for Minimizing Mean Square Error\n",
        "\n",
        "# Input argument is weight and a tuple (train_data, target)\n",
        "def grad_mse(w,xy):\n",
        "    (x,y) = xy\n",
        "    (rows,cols) = x.shape\n",
        "    \n",
        "    # Compute the output\n",
        "    o = np.sum(x*w,axis=1)\n",
        "    diff = y-o\n",
        "    diff = diff.reshape((rows,1))    \n",
        "    diff = np.tile(diff, (1, cols))\n",
        "    grad = diff*x\n",
        "    grad = -np.sum(grad,axis=0)\n",
        "    return grad\n",
        "\n",
        "# Input argument is weight and a tuple (train_data, target)\n",
        "def mse(w,xy):\n",
        "    (x,y) = xy\n",
        "    \n",
        "    # Compute output\n",
        "    # keep in mind that wer're using mse and not mse/m\n",
        "    # because it would be relevant to the end result\n",
        "    o = np.sum(x*w,axis=1)\n",
        "    mse = np.sum((y-o)*(y-o))\n",
        "    mse = mse/2\n",
        "    return mse    \n",
        "\n",
        "\n",
        "\n",
        "# Running Gradient Descent on OCR\n",
        "\n",
        "# Load the digits dataset with two classes\n",
        "digits,target = dt.load_digits(n_class=2,return_X_y=True)\n",
        "fig,ax = plt.subplots(nrows=1, ncols=10,figsize=(12,4),subplot_kw=dict(xticks=[], yticks=[]))\n",
        "\n",
        "# Plot some images of digits\n",
        "for i in np.arange(10):\n",
        "    ax[i].imshow(digits[i,:].reshape(8,8),cmap=plt.cm.gray)   \n",
        "plt.show()\n",
        "\n",
        "# Split into train and test set\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "                        digits, target, test_size=0.2, random_state=10)\n",
        "\n",
        "# Add a column of ones to account for bias in train and test\n",
        "x_train = np.hstack((np.ones((y_train.size,1)),x_train))\n",
        "x_test  = np.hstack((np.ones((y_test.size,1)),x_test))\n",
        "\n",
        "\n",
        "# Function that calculates the history of the NAG\n",
        "def nag(max_epochs, threshold, w_init, obj_func, grad_func, xy, mu=0.1,\n",
        "        learning_rate=0.05, momentum=0.8):\n",
        "    (x_train,y_train) = xy\n",
        "    w = w_init\n",
        "    w_history = w\n",
        "    f_history = obj_func(w, xy)\n",
        "    delta_w = np.zeros(w.shape)\n",
        "    i = 0\n",
        "    diff = 1.0e10\n",
        "    \n",
        "    while i < max_epochs and diff > threshold:\n",
        "        # Calculate gradient with lookahead weights\n",
        "        lookahead_w = w + momentum * delta_w * mu\n",
        "        lookahead_gradient = grad_func(lookahead_w, xy)\n",
        "        \n",
        "        delta_w = -learning_rate * lookahead_gradient\n",
        "        w += mu*delta_w\n",
        "        \n",
        "        # Store the history of w and f\n",
        "        w_history = np.vstack((w_history, w))\n",
        "        f_history = np.vstack((f_history, obj_func(w,xy)))\n",
        "        \n",
        "        # Update iteration number and diff between successive values\n",
        "        i += 1\n",
        "        diff = np.absolute(f_history[-1] - f_history[-2])\n",
        "        \n",
        "    return w_history, f_history\n",
        "\n",
        "\n",
        "def visualize(w_history):\n",
        "    # Make the function plot\n",
        "    function_plot(pts, f_vals)\n",
        "    \n",
        "    # Plot the history\n",
        "    plt.plot(w_history[:, 0], w_history[:, 1], marker='o', c='magenta')\n",
        "    \n",
        "    # Annotate the point found at last iteration\n",
        "    annotate_pt('minimum found', (w_history[-1, 0], w_history[-1, 1]), (-1, 7), 'green')\n",
        "    \n",
        "    iter = w_history.shape[0]\n",
        "    for w, i in zip(w_history, range(iter-1)):\n",
        "        # Annotate with arrows to show history\n",
        "        plt.annotate(\"\",\n",
        "                     xy=w, xycoords='data',\n",
        "                     xytext=w_history[i+1, :], textcoords='data',\n",
        "                     arrowprops=dict(arrowstyle='<-',\n",
        "                                     connectionstyle='angle3'))\n",
        "\n",
        "# Function that calculates different mu of NAG\n",
        "def nag_mu(mu, i):\n",
        "  plt.subplot(3,4,i+1)\n",
        "  xy = (pts, f_vals)\n",
        "  w_init = np.array([10.0, 10.0])  # Initial weights\n",
        "  max_epochs = 100  # Maximum number of epochs\n",
        "  threshold = 1e-8  # Threshold for convergence\n",
        "  w_history, f_history = nag(max_epochs, threshold, w_init, f, grad, xy, mu,\n",
        "                            learning_rate=0.05, momentum=0.8)\n",
        "  visualize(w_history)\n",
        "  plt.title('NAG mu = ' + str(mu))\n",
        "  # plt.show()\n",
        "\n",
        "# The part that outputs all records according to the parameters\n",
        "fig, ax = plt.subplots(nrows=4, ncols=4, figsize=(18, 12))\n",
        "mu = 0\n",
        "for i in range(0,11):\n",
        "  nag_mu(mu, i)\n",
        "\n",
        "  mu += 0.1\n",
        "  mu = round(mu, 1)\n",
        "\n",
        "plt.suptitle('NAG',fontsize=18)\n",
        "plt.show()\n"
      ]
    }
  ]
}